import sys
import os

# Add the project root to Python path
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))
'''
Basic Motive is to check 
Langchain response time and langgraph response

same prompt but there 5 prompt one bu one
this should be sequencial not parallel execution

same llm configuration
'''

promps=[
    "Hi",
    "How are you today?",
    "What is AI",
    "Who is the father of AI",
    "What is the difference between AI and ML"
]



### llm configuration
temperature=0.9
max_token=200

from chatbot.langchain_function import langchainChatbot
from chatbot.langgraph_function import langGraphChatbot

LC_response_time=[]
LG_response_time=[]

import time
for prompt in promps:
    print("Lang Chain Start")
    stime=time.time()
    langchain_response=langchainChatbot(temperature=temperature, max_output_tokens=max_token, query=prompt)
    print("Langchain response done .............")
    etime=time.time()
    LC_response_time.append((etime-stime))
    print("Lang Graph Start")
    stime=time.time()
    langGraph_response=langGraphChatbot(temperature=temperature, max_output_tokens=max_token, query=prompt)
    etime=time.time()
    LG_response_time.append((etime-stime))
    print("Graph response done .............")


print("LangChain Time: ", LC_response_time)
print("LangGraph Time: ", LG_response_time)



'''
LangChain Time:  [2.7755322456359863, 4.624027490615845, 11.671765804290771, 46.04394769668579, 10.827744960784912]
LangGraph Time:  [4.013000249862671, 4.660099506378174, 11.164138793945312, 6.539490699768066, 12.58867883682251]



Main Reasons for Time Variations
1. Network Latency

Each API call goes through the internet to Google's servers
Network conditions vary: congestion, routing, packet loss
Distance to the nearest Google API server can fluctuate

2. API Server Load

Google's servers handle millions of requests
Server load varies based on:

Time of day
Other users' concurrent requests
Server availability and distribution



3. LLM Processing Complexity

Different prompts require different processing:

"Hi" ‚Üí Very simple, quick response
"What is the difference between AI and ML" ‚Üí More complex reasoning, longer generation time


Token generation is sequential, so longer responses take more time

4. Cold Start vs Warm Requests

First request might initialize connections
Subsequent requests may benefit from cached connections
But this can vary unpredictably

5. LangGraph Overhead

LangGraph adds a graph compilation and execution layer
This overhead is relatively consistent but adds to total time
Sometimes this overhead is offset by other factors being faster

Your Specific Results Analysis
Prompt 4: "Who is the father of AI"
- LangChain: 46.04s (unusually slow!)
- LangGraph: 6.54s (normal)
This massive spike (46 seconds) suggests:

Network timeout/retry
Server-side queueing
Temporary API slowdown

How to Get More Consistent Results
pythonimport time
import statistics

def benchmark_with_multiple_runs(prompt, runs=3):
    """Run same prompt multiple times and get statistics"""
    lc_times = []
    lg_times = []
    
    for i in range(runs):
        # LangChain
        start = time.time()
        langchainChatbot(temperature=0.9, max_output_tokens=200, query=prompt)
        lc_times.append(time.time() - start)
        
        time.sleep(1)  # Small delay between calls
        
        # LangGraph
        start = time.time()
        langGraphChatbot(temperature=0.9, max_output_tokens=200, query=prompt)
        lg_times.append(time.time() - start)
        
        time.sleep(1)
    
    return {
        'lc_avg': statistics.mean(lc_times),
        'lc_median': statistics.median(lc_times),
        'lc_stdev': statistics.stdev(lc_times) if len(lc_times) > 1 else 0,
        'lg_avg': statistics.mean(lg_times),
        'lg_median': statistics.median(lg_times),
        'lg_stdev': statistics.stdev(lg_times) if len(lg_times) > 1 else 0,
    }

# Test with multiple runs
results = benchmark_with_multiple_runs("What is AI", runs=5)
print(f"LangChain - Avg: {results['lc_avg']:.2f}s, Median: {results['lc_median']:.2f}s")
print(f"LangGraph - Avg: {results['lg_avg']:.2f}s, Median: {results['lg_median']:.2f}s")
Expected Performance Pattern
For a simple chatbot like yours:

LangChain: Should be slightly faster (less overhead)
LangGraph: Adds ~0.5-2s overhead for graph compilation/execution



######## ###########################################################
‡¶§‡¶æ‡¶∞ ‡¶Æ‡¶æ‡¶®‡ßá ‡¶Ü‡¶Æ‡¶æ‡¶∞ ‡¶•‡ßá‡¶ï‡ßá fast ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶ï‡ßã‡¶®‡¶ì ‡¶ï‡¶ø‡¶õ‡ßÅ ‡¶®‡ßá‡¶á‡•§ ‡¶Æ‡¶æ‡¶®‡ßá ‡¶è‡¶á ‡¶Æ‡ßá‡¶•‡¶°‡ßá ‡¶Ü‡¶Æ‡¶ø LangChain LangGraph ‡¶Ø‡¶æ‡¶á ‡¶á‡ßü‡¶π‡¶æ‡¶∞ ‡¶ï‡¶ì‡¶∞‡¶á almost same response ‡¶Ü‡¶∏‡¶¨‡ßá??4:57 PM‡¶π‡ßç‡¶Ø‡¶æ‡¶Å, ‡¶è‡¶ï‡¶¶‡¶Æ ‡¶†‡¶ø‡¶ï ‡¶¨‡¶≤‡ßá‡¶õ‡ßá‡¶®!
‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ Current Use Case ‡¶è:
‡¶Ü‡¶™‡¶®‡¶ø ‡¶Ø‡ßá‡¶π‡ßá‡¶§‡ßÅ ‡¶∂‡ßÅ‡¶ß‡ßÅ simple single-turn chatbot ‡¶¨‡¶æ‡¶®‡¶æ‡¶ö‡ßç‡¶õ‡ßá‡¶® (‡¶è‡¶ï‡¶ü‡¶æ prompt ‚Üí ‡¶è‡¶ï‡¶ü‡¶æ response), ‡¶∏‡ßá‡¶ñ‡¶æ‡¶®‡ßá:

‚úÖ LangChain ‡¶è‡¶¨‡¶Ç LangGraph ‡¶™‡ßç‡¶∞‡¶æ‡¶Ø‡¶º same performance ‡¶¶‡ßá‡¶¨‡ßá
‚úÖ ‡¶Ü‡¶∏‡¶≤ ‡¶∏‡¶Æ‡¶Ø‡¶º ‡¶≤‡¶æ‡¶ó‡¶õ‡ßá Google API call ‡¶è (network + LLM processing)
‚úÖ Framework ‡¶è‡¶∞ overhead ‡¶Æ‡¶æ‡¶§‡ßç‡¶∞ 0.5-2 seconds, ‡¶Ø‡¶æ negligible

‡¶ï‡¶ñ‡¶® LangGraph ‡¶¶‡¶∞‡¶ï‡¶æ‡¶∞?
LangGraph ‡¶§‡¶ñ‡¶®‡¶á ‡¶¶‡¶∞‡¶ï‡¶æ‡¶∞ ‡¶Ø‡¶ñ‡¶® ‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ complex workflow ‡¶Ü‡¶õ‡ßá:
‚ùå ‡¶¶‡¶∞‡¶ï‡¶æ‡¶∞ ‡¶®‡ßá‡¶á (‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ case):
python# Simple: User ‚Üí LLM ‚Üí Response
user_input ‚Üí llm.invoke() ‚Üí response
‚úÖ ‡¶¶‡¶∞‡¶ï‡¶æ‡¶∞ ‡¶Ü‡¶õ‡ßá:
python# Complex workflow ‡¶â‡¶¶‡¶æ‡¶π‡¶∞‡¶£:

1. Multi-step reasoning:
   user_input ‚Üí research_node ‚Üí analyze_node ‚Üí summarize_node ‚Üí response

2. Conditional logic:
   user_input ‚Üí classify_node 
              ‚îú‚Üí technical_question ‚Üí code_node
              ‚îî‚Üí general_question ‚Üí chat_node

3. Agent with tools:
   user_input ‚Üí agent_node ‚Üí [web_search, calculator, database]
              ‚Üí combine_results ‚Üí response

4. Human-in-the-loop:
   user_input ‚Üí draft_node ‚Üí wait_for_approval ‚Üí finalize_node
‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ Code ‡¶è‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø Recommendation:
‡¶∂‡ßÅ‡¶ß‡ßÅ LangChain ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßÅ‡¶®! ‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ ‡¶™‡ßç‡¶∞‡¶•‡¶Æ code ‡¶ü‡¶æ‡¶á ‡¶Ø‡¶•‡ßá‡¶∑‡ßç‡¶ü:
pythondef langchainChatbot(temperature, max_output_tokens, query) -> str: 
    llm = ChatGoogleGenerativeAI(
        model="gemini-2.5-flash",
        temperature=temperature,
        max_output_tokens=max_output_tokens,
    )
    response = llm.invoke(query)
    return response.content
‡¶è‡¶ü‡¶æ‡¶á fastest ‡¶è‡¶¨‡¶Ç simplest ‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ use case ‡¶è‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø‡•§
Response Time Fast ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶â‡¶™‡¶æ‡¶Ø‡¶º:
‡¶Ø‡¶¶‡¶ø ‡¶Ü‡¶™‡¶®‡¶ø ‡¶∏‡¶§‡ßç‡¶Ø‡¶ø‡¶á faster response ‡¶ö‡¶æ‡¶®:
1. Streaming ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßÅ‡¶®:
pythondef langchainChatbot_streaming(temperature, max_output_tokens, query):
    llm = ChatGoogleGenerativeAI(
        model="gemini-2.5-flash",
        temperature=temperature,
        max_output_tokens=max_output_tokens,
    )
    
    # User ‡¶ï‡ßá real-time ‡¶è response ‡¶¶‡ßá‡¶ñ‡¶æ‡¶® (ChatGPT ‡¶è‡¶∞ ‡¶Æ‡¶§‡ßã)
    for chunk in llm.stream(query):
        print(chunk.content, end="", flush=True)
‡¶è‡¶§‡ßá user ‡¶Æ‡¶®‡ßá ‡¶ï‡¶∞‡¶¨‡ßá response faster ‡¶Ü‡¶∏‡¶õ‡ßá!
2. Faster Model ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßÅ‡¶®:
pythonmodel="gemini-2.0-flash-exp"  # ‡¶Ü‡¶∞‡¶ì fast
3. Caching (‡¶Ø‡¶¶‡¶ø same question ‡¶¨‡¶æ‡¶∞‡¶¨‡¶æ‡¶∞ ‡¶Ü‡¶∏‡ßá):
pythoncache = {}

def langchainChatbot_cached(temperature, max_output_tokens, query):
    if query in cache:
        return cache[query]  # Instant response!
    
    llm = ChatGoogleGenerativeAI(...)
    response = llm.invoke(query)
    cache[query] = response.content
    return response.content
Summary:

üéØ ‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ current use case ‡¶è: LangChain ‡¶Ø‡¶•‡ßá‡¶∑‡ßç‡¶ü, LangGraph ‡¶¶‡¶∞‡¶ï‡¶æ‡¶∞ ‡¶®‡ßá‡¶á
‚ö° Fast ‡¶ï‡¶∞‡¶§‡ßá ‡¶ö‡¶æ‡¶á‡¶≤‡ßá: Streaming ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßÅ‡¶®
üîÑ Complex workflow ‡¶•‡¶æ‡¶ï‡¶≤‡ßá: ‡¶§‡¶ñ‡¶® LangGraph ‡¶è ‡¶Ø‡¶æ‡¶¨‡ßá‡¶®

‡¶Ü‡¶∂‡¶æ ‡¶ï‡¶∞‡¶ø clear ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá! ‡¶Ü‡¶∞‡¶ì ‡¶ï‡¶ø‡¶õ‡ßÅ ‡¶ú‡¶æ‡¶®‡¶§‡ßá ‡¶ö‡¶æ‡¶á‡¶≤‡ßá ‡¶¨‡¶≤‡¶¨‡ßá‡¶®‡•§ üòä
'''